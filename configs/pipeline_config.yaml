pipeline:
  - name: load_data
    class: src.components.loader.DataLoader
    method: load_from_pdf
    params:
      metadata: {}
    input:
      pdf_path: "data/pdfs/wp_generative_ai_risk_management_in_fs.pdf"
      split_from_mid: True
    output: docs

  - name: chunking
    class: src.components.chunker.TextSplitter
    method: split
    params:
      chunk_size: 1000
      chunk_overlap: 100
    input:
      docs: docs
    output: split_docs

  - name: embedding
    class: src.components.embedder.DocEmbedder
    method: embed
    params:
      model_name: "all-MiniLM-L6-v2"
      vs_name: "faiss_index_google_genai_risk_mgmt"
    input:
      split_docs: split_docs
      persist_vs: "serverless"
      tablename: "faiss_index_google_genai_risk_mgmt"
    output: vectorstore

  - name: retrieving
    class: src.components.retriever.PostgresRetriever
    method: retrieve
    params:
      tablename: "faiss_index_google_genai_risk_mgmt"
      embedding_model_name: "all-MiniLM-L6-v2"
    input:
      query:
      k_retrieval: 20
    output: retrieved_docs

  - name: reranking
    class: src.components.reranker.Reranker
    method: rerank
    params:
      k_rerank: 5 # choose top k reranker score
      model_rerank: "gemini-2.5-flash"
      model_rerank_provider: "google_genai"
      temperature_rerank: 0
      top_k_rerank: 5 # top k token in generation of the rerank score
      top_p_rerank: 0.8
      prompt_name: "system-reranker-prompt"
      prompt_version: "latest"
    input:
      query:
      retrieved_docs: retrieved_docs
    output: reranked_document

  - name: autorating
    class: src.components.autorater.Autorater
    method: autorate
    params:
      model_autorate: "gemini-2.5-flash"
      model_autorate_provider: "google_genai"
      temperature_autorate: 0
      top_k_autorate: 5
      top_p_autorate: 0.8
      prompt_name: "system-autorater-prompt"
      prompt_version: "latest"
    input:
      query:
      reranked_document: reranked_document
    output: context

  - name: generating
    class: src.components.generator.GeneratorLLM
    method: generate
    params:
      model: "gemini-2.5-flash"
      model_provider: "google_genai"
      temperature: 0
      top_k: 5 # top k token in generation of the rerank score
      top_p: 0.9
      prompt_name: "system-system-prompt"
      prompt_version: "latest"
    input:
      context: context
      question:
    output:

