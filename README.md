# RAG

## Introduction

In this repo, I have developed a production-level, modulized RAG chatbot. In MVP1, user can ask questions about a 
topic from the Wikipedia page. In future MVPs, the plan is to develop a chatbot that reference difference sources. 
This can easily be done due to the highly modulized structure of the code.

**Highlight**: 

the structure of the repo allows for easy customized experimentation, including addition of new components,
  parameter tuning, prompt engineering etc. Using the langchain tech stack provides a seamless connection to the
  langsmith UI for tracing and evaluation capabilities.

## Tech stack
- **Langchain**, specifically **LCEL** (LangChain Expression Language) as the main development framework.
  Modularity is further enhanced using **dynamic chaining**
- **Langsmith** for prompt versioning, tracing, evaluation and experimentation. With subscription, one can deploy
  the RAG as runnable in Langsmith and experiment with different prompts using the **playground** feature

## Instructions
In MVP1, there are the following components for the RAG under `src/components`
- `loader.py`: make API call to Wikipedia and fetch the article of the given topic. Can expand capability to fetch 
  other document sources
- `chunker.py`: recursively chunk the document. Can expand to other more sophisticated chunking techniques
- `embedder.py`: embedding chunks into the FAISS vector store. Can expand to other vector store such as *weaviate*
  that support hybrid search (cosine similarity + BM25)
- `retriever.py`: retrieve top chunks given the user query
- `reranker.py`: rerank retrieved chunks by their relevancy to the user query using a LLM
- `autorater.py`: autorate the reranked chunks by determining if they are contextually sufficient to answer
   the query question. This helps abstain answering un-answerable questions
- `generator.py`: generate the final answer based on the reranked, retrieved chunks and the user query
 
All components are highly modulized. User specifies the parameters of each component and how they link together
in `configs/pipeline_config.yaml`. The `src/components/runner.py` script call the `chain_from_yaml` method to
build the LCEL chain based on the config yaml file.

User can choose to persist it with the `persist_vs=True` option in the embedder. Once the vector store is built,
one can use `configs/inference_pipeline_config.yaml`, which starts from retreival from the vector store.

All LLM are initialized by the `init_chat_model` method from langchain. Inference is made via making API call
to the selected model (gemini-2.5-flash for MVP1)

### Evaluation
User can specify examples of query-gold response pair in `data/evaluation/eval_examples.json` for evaluation. 
Once examples are given, run `evaluations/eval.py`, which uses LLM-as-a-Judge to determine if the response 
generated by each query example are similar to the given gold response.

In MVP2, an autorater is added to abstain answering non-answerable questions. User is encouraged to
add unanswerable query in `eval_examples.json` to achieve a more comprehensive evaluation.

## MVP2

- ~~Add abstention~~
- Add citation  
- Add metadata to document



