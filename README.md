# RAG (MVP2)

## Intoduction
In this repo, I have developed a production-level, modulized RAG chatbot. user can ask questions from a given
pdf document.  

**Highlight**

The structure of the repo allows for easy customized experimentation, including addition of new components,
parameter tuning, prompt engineering etc. The structure also enables easy setup of unit tests. The repo also
contains tracing and versioning capability, enabling rigorous experimentation and iterations.

[**MVP2 Update**] langsmith is no longer used for experimentation and evaluation. We have switched to
*mlflow* instead.

## Tech stack
- **Langchain**, specifically **LCEL** (LangChain Expression Language) as the main development framework.
  Modularity is further enhanced using **dynamic chaining**
- **mlflow** is used for experimentation and model versioning in MVP2  

---

# \*\*MVP 1 README blow\*\*

## Introduction

In this repo, I have developed a production-level, modulized RAG chatbot. In MVP1, user can ask questions about a 
topic from the Wikipedia page. In future MVPs, the plan is to develop a chatbot that reference difference sources. 
This can easily be done due to the highly modulized structure of the code.

**Highlight**: 

the structure of the repo allows for easy customized experimentation, including addition of new components,
  parameter tuning, prompt engineering etc. Using the langchain tech stack provides a seamless connection 
to the langsmith UI for tracing and evaluation capabilities.

## Tech stack
- **Langchain**, specifically **LCEL** (LangChain Expression Language) as the main development framework.
  Modularity is further enhanced using **dynamic chaining**
- **Langsmith** [*sunset in MVP2] for prompt versioning, tracing, evaluation and experimentation. With subscription, 
  one can deploy the RAG as runnable in Langsmith and experiment with different prompts using the 
  **playground** feature

## Instructions
In MVP1, there are the following components for the RAG under `src/components`
- `loader.py`: make API call to Wikipedia and fetch the article of the given topic. Can expand capability to fetch 
  other document sources
- `chunker.py`: recursively chunk the document. Can expand to other more sophisticated chunking techniques
- `embedder.py`: embedding chunks into the FAISS vector store. Can expand to other vector store such as *weaviate*
  that support hybrid search (cosine similarity + BM25)
- `retriever.py`: retrieve top chunks given the user query
- `reranker.py`: rerank retrieved chunks by their relevancy to the user query using a LLM
- `autorater.py`: autorate the reranked chunks by determining if they are contextually sufficient to answer
   the query question. This helps abstain answering un-answerable questions
- `generator.py`: generate the final answer based on the reranked, retrieved chunks and the user query
 
All components are highly modulized. User specifies the parameters of each component and how they link together
in `configs/pipeline_config.yaml`. The `src/components/runner.py` script call the `chain_from_yaml` method to
build the LCEL chain based on the config yaml file.

User can choose to persist it with the `persist_vs=True` option in the embedder. Once the vector store is built,
one can use `configs/inference_pipeline_config.yaml`, which starts from retreival from the vector store.

All LLM are initialized by the `init_chat_model` method from langchain. Inference is made via making API call
to the selected model (gemini-2.5-flash for MVP1)

**Note on `pipeline_config.yaml` format**

The second to last component output variable should be named `context`. The last component,
which is the LLM generator, must have `context` and `question` as the two inputs. This is to
make sure it follows the variable names in the generator prompt. Also the output of the last
component must be empty.

### Evaluation
User can specify examples of query-gold response pair in `data/evaluation/eval_examples.json` for evaluation. 
Once examples are given, run `evaluations/eval.py`, which uses LLM-as-a-Judge to determine if the response 
generated by each query example are similar to the given gold response.

In MVP2, an autorater is added to abstain answering non-answerable questions. User is encouraged to
add unanswerable query in `eval_examples.json` to achieve a more comprehensive evaluation.

### Experimentation
To log the `RunnableSequence` using mlflow, one needs two scripts: 1) set the model from script using
`mlflow.models.set_model(chain)` and 2) to log the chain in mlflow from the the first script using
`mlflow.langchain.log_model(lc_model=<first script name>)`. The reason of setting model from script is
to avoid serializing (aka pickling) the chain as some components are not serializable. In the root directory,
run `python -m src.mlflow.log_model` (this script includes running `subprocess` that will run 
`python -m src.mlflow.set_model` first.

Note that the artifacts saved will not capture the parameters specified by the yaml files in `configs`
To log the parameters, `mlflow.log_param()` or `mlflow.log_artifact()` is needed. Also, to register the prompts,
run `python -m src.register_prompt`

All together, run the following command in terminal:

`python -m src.mlflow.register_prompt && python -m src.mlflow.log_model`

Run `python -m experiments.run_experiment` to run experiment on test examples specified in `data/evaluation`, tracing,
and evaluation.

## MVP2

- ~~Add abstention~~
- ~~Add citation~~  
- mlflow
  - ~~set, log and register model~~
  - ~~experiments and runs~~
  - enable tracing (version issue)
- ~~Add metadata to document~~
- LLM-as-a-Judge (hallucination, correctness, retrieval groundedness)
- ~~add pdf reader: domain expert RAG~~
- serverless storage pgvector (neon)

## MVP3

- Agentic RAG (modulized)
- Graph RAG


